{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnsit = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into train, test and validation\n",
    "train_size = int(0.8 * len(mnsit))\n",
    "test_size = len(mnsit) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    mnsit, [train_size, test_size]\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = torch.nn.Linear(32 * 7 * 7, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[ 0.2263,  0.0876, -0.0450],\n",
       "                        [-0.3229,  0.0884, -0.1193],\n",
       "                        [ 0.2156, -0.1474, -0.2008]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2970,  0.1203,  0.2262],\n",
       "                        [ 0.2009,  0.2276, -0.0481],\n",
       "                        [-0.2551,  0.0271, -0.2636]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1325, -0.3077,  0.0899],\n",
       "                        [ 0.3202, -0.0005,  0.1568],\n",
       "                        [ 0.3011, -0.0493,  0.1492]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2841,  0.1373, -0.2483],\n",
       "                        [ 0.0939, -0.1654, -0.0769],\n",
       "                        [ 0.2735,  0.3103, -0.0568]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2746, -0.2575,  0.1935],\n",
       "                        [-0.3154,  0.1762,  0.1813],\n",
       "                        [ 0.0395, -0.2632,  0.2744]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2079, -0.2383,  0.1862],\n",
       "                        [-0.1642, -0.0061,  0.1316],\n",
       "                        [-0.2012,  0.1964, -0.0678]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0230,  0.3098,  0.0967],\n",
       "                        [-0.0055, -0.3231,  0.0736],\n",
       "                        [-0.3190,  0.0810,  0.3258]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2828, -0.2427,  0.1857],\n",
       "                        [-0.2730,  0.0385, -0.0102],\n",
       "                        [-0.1483,  0.0234, -0.2211]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3284, -0.2936, -0.2403],\n",
       "                        [-0.1068, -0.2178, -0.0901],\n",
       "                        [ 0.0288,  0.2380,  0.0670]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0988,  0.2498,  0.2798],\n",
       "                        [-0.1645, -0.2147, -0.0110],\n",
       "                        [ 0.0103, -0.1958, -0.1565]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0103, -0.2186, -0.2462],\n",
       "                        [-0.0741,  0.2075, -0.2864],\n",
       "                        [ 0.0666,  0.3327, -0.2675]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0991,  0.0711,  0.1086],\n",
       "                        [-0.0811, -0.1584, -0.1056],\n",
       "                        [ 0.2943,  0.1712,  0.2067]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2636,  0.2434, -0.1137],\n",
       "                        [-0.2419,  0.2583, -0.3236],\n",
       "                        [-0.1758, -0.0860, -0.3053]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2311,  0.2011,  0.1623],\n",
       "                        [ 0.2511,  0.1786,  0.3267],\n",
       "                        [-0.0227, -0.2181,  0.2874]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1793,  0.3240,  0.1562],\n",
       "                        [-0.2643,  0.2610,  0.2630],\n",
       "                        [-0.1865, -0.1312,  0.1953]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0823, -0.1421, -0.1192],\n",
       "                        [ 0.1743, -0.3098,  0.1863],\n",
       "                        [-0.1651, -0.0409,  0.0102]]]])),\n",
       "             ('conv1.bias',\n",
       "              tensor([ 0.2539, -0.0609,  0.1030,  0.2949,  0.0347, -0.1538, -0.0630,  0.2469,\n",
       "                       0.3007,  0.1540,  0.2039,  0.1651,  0.0908, -0.2025,  0.3316,  0.1334])),\n",
       "             ('conv2.weight',\n",
       "              tensor([[[[ 0.0625,  0.0729,  0.0088],\n",
       "                        [ 0.0735, -0.0313, -0.0184],\n",
       "                        [-0.0801,  0.0591,  0.0348]],\n",
       "              \n",
       "                       [[ 0.0462, -0.0383,  0.0698],\n",
       "                        [-0.0711, -0.0159, -0.0180],\n",
       "                        [ 0.0282,  0.0804,  0.0099]],\n",
       "              \n",
       "                       [[-0.0709,  0.0647,  0.0470],\n",
       "                        [-0.0764, -0.0005, -0.0394],\n",
       "                        [ 0.0258,  0.0274, -0.0725]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0306,  0.0353, -0.0388],\n",
       "                        [ 0.0200,  0.0548, -0.0480],\n",
       "                        [ 0.0037,  0.0539,  0.0312]],\n",
       "              \n",
       "                       [[-0.0247,  0.0650,  0.0241],\n",
       "                        [-0.0410,  0.0294,  0.0158],\n",
       "                        [-0.0445,  0.0446, -0.0337]],\n",
       "              \n",
       "                       [[-0.0801, -0.0167,  0.0076],\n",
       "                        [-0.0384,  0.0322, -0.0083],\n",
       "                        [-0.0592, -0.0297, -0.0800]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0759, -0.0231,  0.0032],\n",
       "                        [-0.0510,  0.0495,  0.0271],\n",
       "                        [-0.0776, -0.0197,  0.0484]],\n",
       "              \n",
       "                       [[ 0.0096,  0.0177, -0.0305],\n",
       "                        [-0.0255,  0.0387, -0.0541],\n",
       "                        [ 0.0250, -0.0329,  0.0799]],\n",
       "              \n",
       "                       [[ 0.0816, -0.0123, -0.0409],\n",
       "                        [ 0.0593, -0.0658, -0.0153],\n",
       "                        [ 0.0787, -0.0796, -0.0129]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0818,  0.0088, -0.0448],\n",
       "                        [-0.0818, -0.0220, -0.0755],\n",
       "                        [-0.0323,  0.0216, -0.0762]],\n",
       "              \n",
       "                       [[ 0.0143,  0.0457, -0.0319],\n",
       "                        [ 0.0639, -0.0271, -0.0067],\n",
       "                        [-0.0359, -0.0738,  0.0682]],\n",
       "              \n",
       "                       [[-0.0144,  0.0573,  0.0563],\n",
       "                        [ 0.0680, -0.0085, -0.0726],\n",
       "                        [-0.0293, -0.0502,  0.0745]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0767, -0.0722, -0.0023],\n",
       "                        [ 0.0277, -0.0287,  0.0192],\n",
       "                        [ 0.0166, -0.0747, -0.0646]],\n",
       "              \n",
       "                       [[ 0.0027, -0.0479,  0.0730],\n",
       "                        [ 0.0339,  0.0017,  0.0197],\n",
       "                        [ 0.0436,  0.0127, -0.0209]],\n",
       "              \n",
       "                       [[-0.0696, -0.0750,  0.0491],\n",
       "                        [ 0.0411, -0.0113, -0.0098],\n",
       "                        [-0.0098,  0.0587, -0.0437]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-0.0413, -0.0335,  0.0024],\n",
       "                        [-0.0028,  0.0696,  0.0207],\n",
       "                        [-0.0182, -0.0465,  0.0038]],\n",
       "              \n",
       "                       [[ 0.0714,  0.0459,  0.0763],\n",
       "                        [ 0.0106, -0.0674,  0.0668],\n",
       "                        [-0.0692,  0.0657,  0.0722]],\n",
       "              \n",
       "                       [[-0.0604,  0.0560, -0.0229],\n",
       "                        [ 0.0392,  0.0719,  0.0217],\n",
       "                        [-0.0714, -0.0728,  0.0065]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-0.0602,  0.0733,  0.0598],\n",
       "                        [-0.0712, -0.0353, -0.0157],\n",
       "                        [-0.0688, -0.0561, -0.0233]],\n",
       "              \n",
       "                       [[-0.0040,  0.0173,  0.0726],\n",
       "                        [-0.0465, -0.0084, -0.0216],\n",
       "                        [-0.0586,  0.0790,  0.0538]],\n",
       "              \n",
       "                       [[ 0.0532,  0.0495,  0.0092],\n",
       "                        [ 0.0687, -0.0085, -0.0329],\n",
       "                        [-0.0404, -0.0437, -0.0044]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0655,  0.0616,  0.0432],\n",
       "                        [-0.0752,  0.0479,  0.0726],\n",
       "                        [-0.0497, -0.0347, -0.0358]],\n",
       "              \n",
       "                       [[ 0.0494,  0.0248, -0.0613],\n",
       "                        [-0.0386,  0.0082, -0.0490],\n",
       "                        [-0.0618, -0.0522,  0.0744]],\n",
       "              \n",
       "                       [[ 0.0211,  0.0105, -0.0018],\n",
       "                        [-0.0547,  0.0795, -0.0549],\n",
       "                        [-0.0691,  0.0403, -0.0256]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0730, -0.0341,  0.0745],\n",
       "                        [-0.0229, -0.0723, -0.0787],\n",
       "                        [ 0.0166, -0.0315,  0.0007]],\n",
       "              \n",
       "                       [[-0.0327, -0.0587, -0.0795],\n",
       "                        [ 0.0132,  0.0622, -0.0355],\n",
       "                        [-0.0110,  0.0053,  0.0758]],\n",
       "              \n",
       "                       [[-0.0655, -0.0623,  0.0127],\n",
       "                        [ 0.0596,  0.0829,  0.0096],\n",
       "                        [ 0.0388, -0.0456, -0.0041]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0020,  0.0773,  0.0203],\n",
       "                        [-0.0646, -0.0074,  0.0408],\n",
       "                        [ 0.0088, -0.0307, -0.0715]],\n",
       "              \n",
       "                       [[-0.0584, -0.0694,  0.0404],\n",
       "                        [ 0.0225,  0.0168, -0.0545],\n",
       "                        [ 0.0519, -0.0232, -0.0367]],\n",
       "              \n",
       "                       [[ 0.0134,  0.0381,  0.0007],\n",
       "                        [-0.0430,  0.0287,  0.0240],\n",
       "                        [ 0.0481,  0.0138,  0.0198]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0311,  0.0385,  0.0077],\n",
       "                        [ 0.0442,  0.0361, -0.0399],\n",
       "                        [ 0.0616, -0.0494, -0.0728]],\n",
       "              \n",
       "                       [[-0.0113,  0.0397, -0.0655],\n",
       "                        [ 0.0488,  0.0437, -0.0770],\n",
       "                        [ 0.0139, -0.0508,  0.0626]],\n",
       "              \n",
       "                       [[ 0.0258, -0.0696,  0.0609],\n",
       "                        [ 0.0689, -0.0348,  0.0183],\n",
       "                        [ 0.0340,  0.0217,  0.0486]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 0.0494,  0.0169, -0.0708],\n",
       "                        [-0.0234, -0.0170, -0.0317],\n",
       "                        [-0.0608,  0.0318, -0.0259]],\n",
       "              \n",
       "                       [[-0.0345,  0.0091, -0.0708],\n",
       "                        [ 0.0046, -0.0492,  0.0395],\n",
       "                        [-0.0797, -0.0211,  0.0369]],\n",
       "              \n",
       "                       [[ 0.0720,  0.0629,  0.0823],\n",
       "                        [ 0.0829, -0.0287,  0.0340],\n",
       "                        [-0.0460,  0.0736,  0.0820]]]])),\n",
       "             ('conv2.bias',\n",
       "              tensor([ 0.0302,  0.0815, -0.0613,  0.0734,  0.0319, -0.0726, -0.0262, -0.0502,\n",
       "                      -0.0586,  0.0640, -0.0643, -0.0747,  0.0593,  0.0383,  0.0375, -0.0246,\n",
       "                       0.0371, -0.0140,  0.0276,  0.0329, -0.0135, -0.0437,  0.0729, -0.0033,\n",
       "                      -0.0483,  0.0371,  0.0044, -0.0346, -0.0198, -0.0686, -0.0639, -0.0746])),\n",
       "             ('fc1.weight',\n",
       "              tensor([[-2.4939e-02, -1.0023e-02, -3.9249e-03,  ..., -1.9297e-02,\n",
       "                       -3.7287e-03,  1.7657e-03],\n",
       "                      [-8.0669e-03,  1.4264e-05, -1.8712e-03,  ..., -8.7245e-03,\n",
       "                        1.6262e-02, -1.4275e-02],\n",
       "                      [-2.2383e-02, -1.0276e-03,  1.6173e-02,  ..., -2.1816e-02,\n",
       "                        1.2294e-02, -2.3104e-02],\n",
       "                      ...,\n",
       "                      [-2.0497e-02,  6.3624e-03,  1.7124e-02,  ..., -3.5921e-04,\n",
       "                        1.1158e-02,  8.8312e-03],\n",
       "                      [ 1.8590e-02, -1.7719e-02, -9.0316e-04,  ...,  2.1561e-02,\n",
       "                       -1.0116e-02,  1.0743e-02],\n",
       "                      [ 1.4314e-02,  1.9356e-02,  2.1089e-02,  ..., -1.1599e-02,\n",
       "                       -1.1594e-02,  2.0866e-02]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([-9.2008e-03,  2.2002e-02, -8.8659e-03, -1.6278e-02, -3.3663e-03,\n",
       "                      -1.4999e-02,  2.1907e-02, -8.9866e-04, -1.2834e-02,  7.6023e-03,\n",
       "                      -1.6264e-02,  8.7868e-03,  5.0930e-03,  2.0710e-02,  2.0672e-02,\n",
       "                      -2.5352e-04,  4.8457e-03, -1.1027e-02, -7.4071e-03, -4.4572e-03,\n",
       "                      -9.0028e-03, -2.2135e-02, -2.4714e-02, -9.1737e-03,  1.1007e-03,\n",
       "                      -1.9943e-02, -2.1426e-02, -3.1688e-03, -7.9914e-03,  4.8262e-03,\n",
       "                       8.5442e-03,  1.8507e-02,  2.3985e-02,  4.9437e-03, -2.3516e-02,\n",
       "                      -1.5785e-03, -2.1671e-02, -2.6599e-03,  1.9677e-02,  1.8747e-03,\n",
       "                      -1.4411e-02,  1.2670e-02, -1.4756e-02,  7.3089e-03, -2.4669e-02,\n",
       "                      -2.4611e-02, -1.4884e-02, -3.2201e-03, -2.0550e-02,  1.4558e-02,\n",
       "                      -6.3549e-03,  1.8966e-02, -1.1480e-02,  1.0274e-02,  1.5601e-02,\n",
       "                      -1.6688e-02, -2.5001e-02, -1.3542e-02, -1.7235e-02, -1.8384e-02,\n",
       "                      -1.9696e-02,  1.9146e-02,  2.5086e-02, -3.7050e-04,  2.2720e-02,\n",
       "                      -1.1514e-02,  4.5008e-03, -1.4637e-02,  3.5600e-03,  1.9802e-02,\n",
       "                      -1.1686e-02,  8.0654e-04, -6.6832e-03, -6.2368e-03, -6.7772e-03,\n",
       "                      -1.5509e-02,  1.6273e-02, -4.5411e-03, -2.4045e-02,  1.5119e-02,\n",
       "                      -1.5222e-02, -1.1046e-03, -1.2073e-02,  4.0331e-03, -2.0262e-02,\n",
       "                      -1.5466e-02,  1.9423e-02, -1.4716e-02,  7.5740e-03, -6.5504e-03,\n",
       "                       4.1514e-03,  1.0387e-02,  4.6047e-03, -2.3585e-02, -5.4653e-03,\n",
       "                      -1.3738e-02,  7.6998e-03,  5.4243e-03, -1.4143e-04,  2.4495e-02,\n",
       "                      -6.3084e-03, -1.8353e-02,  4.2551e-03,  4.6028e-03, -1.0875e-02,\n",
       "                      -6.0318e-04,  1.6588e-03, -2.4006e-02,  7.8784e-03, -1.6641e-02,\n",
       "                      -2.8284e-03,  2.6658e-03, -1.2360e-02, -1.8733e-02,  1.3508e-02,\n",
       "                       1.0284e-02, -2.0550e-02, -2.2425e-02,  7.5737e-03, -1.8031e-02,\n",
       "                       1.9074e-02,  1.4503e-02,  8.8563e-05, -2.1908e-02, -2.0366e-02,\n",
       "                       2.2641e-02, -1.0089e-02, -2.5225e-02])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 0.0635, -0.0251,  0.0261,  ..., -0.0490,  0.0725,  0.0564],\n",
       "                      [ 0.0818, -0.0685,  0.0064,  ...,  0.0227,  0.0028, -0.0506],\n",
       "                      [-0.0594, -0.0701,  0.0006,  ..., -0.0113, -0.0224,  0.0762],\n",
       "                      ...,\n",
       "                      [ 0.0841, -0.0142,  0.0535,  ...,  0.0873,  0.0657,  0.0710],\n",
       "                      [ 0.0757, -0.0061,  0.0173,  ...,  0.0248, -0.0165, -0.0337],\n",
       "                      [ 0.0199, -0.0418, -0.0333,  ...,  0.0431,  0.0036,  0.0546]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([-0.0596,  0.0807, -0.0191, -0.0083,  0.0587,  0.0239, -0.0115, -0.0380,\n",
       "                      -0.0216, -0.0002]))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNNModel()\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_fn, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch: {epoch}, Batch: {i}, Loss: {loss.item()}\")\n",
    "                model.eval()\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for images, labels in test_loader:\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                accuracy = correct / total\n",
    "                print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 2.3035407066345215\n",
      "Accuracy: 0.094\n",
      "Epoch: 0, Batch: 100, Loss: 1.8159127235412598\n",
      "Accuracy: 0.6686666666666666\n",
      "Epoch: 0, Batch: 200, Loss: 1.6802787780761719\n",
      "Accuracy: 0.7594166666666666\n",
      "Epoch: 0, Batch: 300, Loss: 1.6002156734466553\n",
      "Accuracy: 0.8385833333333333\n",
      "Epoch: 0, Batch: 400, Loss: 1.4762157201766968\n",
      "Accuracy: 0.953\n",
      "Epoch: 0, Batch: 500, Loss: 1.4881139993667603\n",
      "Accuracy: 0.9644166666666667\n",
      "Epoch: 0, Batch: 600, Loss: 1.5415012836456299\n",
      "Accuracy: 0.9675833333333334\n",
      "Epoch: 0, Batch: 700, Loss: 1.4800060987472534\n",
      "Accuracy: 0.9675\n",
      "Epoch: 1, Batch: 0, Loss: 1.5271815061569214\n",
      "Accuracy: 0.9703333333333334\n",
      "Epoch: 1, Batch: 100, Loss: 1.5184838771820068\n",
      "Accuracy: 0.9739166666666667\n",
      "Epoch: 1, Batch: 200, Loss: 1.479993462562561\n",
      "Accuracy: 0.96675\n",
      "Epoch: 1, Batch: 300, Loss: 1.4832701683044434\n",
      "Accuracy: 0.9744166666666667\n",
      "Epoch: 1, Batch: 400, Loss: 1.4967296123504639\n",
      "Accuracy: 0.9765\n",
      "Epoch: 1, Batch: 500, Loss: 1.508124589920044\n",
      "Accuracy: 0.9730833333333333\n",
      "Epoch: 1, Batch: 600, Loss: 1.506983995437622\n",
      "Accuracy: 0.9768333333333333\n",
      "Epoch: 1, Batch: 700, Loss: 1.4816054105758667\n",
      "Accuracy: 0.98\n",
      "Epoch: 2, Batch: 0, Loss: 1.478227138519287\n",
      "Accuracy: 0.9804166666666667\n",
      "Epoch: 2, Batch: 100, Loss: 1.468351125717163\n",
      "Accuracy: 0.9811666666666666\n",
      "Epoch: 2, Batch: 200, Loss: 1.5117992162704468\n",
      "Accuracy: 0.9833333333333333\n",
      "Epoch: 2, Batch: 300, Loss: 1.4892104864120483\n",
      "Accuracy: 0.9826666666666667\n",
      "Epoch: 2, Batch: 400, Loss: 1.4779701232910156\n",
      "Accuracy: 0.9813333333333333\n",
      "Epoch: 2, Batch: 500, Loss: 1.4899399280548096\n",
      "Accuracy: 0.9828333333333333\n",
      "Epoch: 2, Batch: 600, Loss: 1.4769901037216187\n",
      "Accuracy: 0.9814166666666667\n",
      "Epoch: 2, Batch: 700, Loss: 1.462205410003662\n",
      "Accuracy: 0.9816666666666667\n",
      "Epoch: 3, Batch: 0, Loss: 1.4772539138793945\n",
      "Accuracy: 0.9769166666666667\n",
      "Epoch: 3, Batch: 100, Loss: 1.5078803300857544\n",
      "Accuracy: 0.9824166666666667\n",
      "Epoch: 3, Batch: 200, Loss: 1.463099479675293\n",
      "Accuracy: 0.9825833333333334\n",
      "Epoch: 3, Batch: 300, Loss: 1.4653857946395874\n",
      "Accuracy: 0.9846666666666667\n",
      "Epoch: 3, Batch: 400, Loss: 1.4767825603485107\n",
      "Accuracy: 0.9833333333333333\n",
      "Epoch: 3, Batch: 500, Loss: 1.4707438945770264\n",
      "Accuracy: 0.9839166666666667\n",
      "Epoch: 3, Batch: 600, Loss: 1.478654384613037\n",
      "Accuracy: 0.9846666666666667\n",
      "Epoch: 3, Batch: 700, Loss: 1.476647138595581\n",
      "Accuracy: 0.9844166666666667\n",
      "Epoch: 4, Batch: 0, Loss: 1.4958226680755615\n",
      "Accuracy: 0.9835\n",
      "Epoch: 4, Batch: 100, Loss: 1.5070679187774658\n",
      "Accuracy: 0.9844166666666667\n",
      "Epoch: 4, Batch: 200, Loss: 1.4738801717758179\n",
      "Accuracy: 0.9864166666666667\n",
      "Epoch: 4, Batch: 300, Loss: 1.464901328086853\n",
      "Accuracy: 0.9861666666666666\n",
      "Epoch: 4, Batch: 400, Loss: 1.4768037796020508\n",
      "Accuracy: 0.9818333333333333\n",
      "Epoch: 4, Batch: 500, Loss: 1.4614369869232178\n",
      "Accuracy: 0.9860833333333333\n",
      "Epoch: 4, Batch: 600, Loss: 1.4766302108764648\n",
      "Accuracy: 0.98175\n",
      "Epoch: 4, Batch: 700, Loss: 1.4668927192687988\n",
      "Accuracy: 0.9863333333333333\n"
     ]
    }
   ],
   "source": [
    "ob = train(model, train_loader, loss_fn, optimizer, epochs=5)\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of weights in the first layer:  144\n"
     ]
    }
   ],
   "source": [
    "model.state_dict()[\"conv1.weight\"].shape\n",
    "print(\"Num of weights in the first layer: \", model.state_dict()[\"conv1.weight\"].numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_population(pruning_percentage, model, population_size):\n",
    "    population = []\n",
    "    for i in range(population_size):\n",
    "        num_weights = model.state_dict()[\"conv1.weight\"].numel()\n",
    "        num_pruned = int(num_weights * pruning_percentage)\n",
    "        mask = torch.ones_like(model.state_dict()[\"conv1.weight\"])\n",
    "\n",
    "        random_indices = torch.randint(0, num_weights, (num_pruned,))\n",
    "        # print(random_indices, i)\n",
    "        mask.view(-1)[random_indices] = 0\n",
    "\n",
    "        population.append(mask)\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5183/1969421861.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[ 0.6052,  0.3859,  0.1449],\n",
       "                        [-0.0127,  0.0975, -0.2710],\n",
       "                        [ 0.1488, -0.3524, -0.4971]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3834,  0.1881,  0.4207],\n",
       "                        [ 0.4898,  0.3829, -0.0098],\n",
       "                        [-0.2248, -0.2529, -0.5597]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0068, -0.7278, -0.2614],\n",
       "                        [ 0.4699,  0.1245,  0.2678],\n",
       "                        [ 0.4007,  0.1198,  0.1814]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.4538,  0.1196, -0.5974],\n",
       "                        [ 0.3765, -0.1184, -0.4085],\n",
       "                        [ 0.3901,  0.4503, -0.1015]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1896, -0.2394,  0.3217],\n",
       "                        [-0.4511,  0.0255,  0.3709],\n",
       "                        [-0.0968, -0.3693,  0.5028]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.3372, -0.2755,  0.4240],\n",
       "                        [-0.3392,  0.1047,  0.3575],\n",
       "                        [-0.1455,  0.4306,  0.0912]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0484,  0.4881,  0.2853],\n",
       "                        [-0.1556, -0.3600,  0.2400],\n",
       "                        [-0.5416, -0.1088,  0.4308]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.5914, -0.0189,  0.2417],\n",
       "                        [-0.2599, -0.0321, -0.1380],\n",
       "                        [-0.2810, -0.1708, -0.4187]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3969, -0.4428, -0.4943],\n",
       "                        [-0.0051, -0.3465, -0.1725],\n",
       "                        [ 0.2314,  0.4339,  0.2520]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1049,  0.4925,  0.5155],\n",
       "                        [-0.2792, -0.2759,  0.1367],\n",
       "                        [-0.2008, -0.3808, -0.2216]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0477, -0.4299, -0.4801],\n",
       "                        [ 0.1099,  0.1369, -0.3685],\n",
       "                        [ 0.3362,  0.6009,  0.0046]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.4508, -0.2106,  0.1037],\n",
       "                        [-0.3064, -0.0345,  0.1808],\n",
       "                        [ 0.5268,  0.5024,  0.3318]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2537,  0.0088, -0.2489],\n",
       "                        [-0.3211,  0.0249, -0.4257],\n",
       "                        [-0.2344, -0.2153, -0.3981]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.4183,  0.3123,  0.2471],\n",
       "                        [ 0.3255,  0.3319,  0.5364],\n",
       "                        [-0.3562, -0.3520,  0.3537]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2280,  0.5142,  0.2236],\n",
       "                        [-0.3882,  0.4071,  0.4464],\n",
       "                        [-0.3233, -0.0711,  0.3568]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0748, -0.1849, -0.2062],\n",
       "                        [-0.0028, -0.3859,  0.1014],\n",
       "                        [-0.2970, -0.0461,  0.1863]]]])),\n",
       "             ('conv1.bias',\n",
       "              tensor([ 0.2244,  0.0307,  0.2489,  0.3578,  0.0882, -0.0837,  0.0524,  0.2178,\n",
       "                       0.2362,  0.0985,  0.1918,  0.3379,  0.0590, -0.0267,  0.4517,  0.0740])),\n",
       "             ('conv2.weight',\n",
       "              tensor([[[[ 1.8582e-02,  1.4070e-02,  5.2688e-02],\n",
       "                        [-8.1987e-02, -1.4459e-01, -1.0740e-01],\n",
       "                        [-1.1191e-01,  3.0214e-03,  8.4169e-02]],\n",
       "              \n",
       "                       [[ 3.9119e-02,  2.7917e-02,  7.8325e-02],\n",
       "                        [-1.3088e-01, -6.0702e-02,  6.8219e-02],\n",
       "                        [ 5.9520e-02,  1.4023e-01,  1.4741e-01]],\n",
       "              \n",
       "                       [[-7.0840e-02,  5.7392e-02,  9.6698e-02],\n",
       "                        [-4.4544e-02, -1.9239e-03,  4.0473e-02],\n",
       "                        [ 1.9330e-01,  1.1502e-01, -1.2376e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.9749e-02,  9.8949e-02,  4.1605e-02],\n",
       "                        [-2.2428e-02,  6.9568e-02,  2.1763e-02],\n",
       "                        [ 7.9642e-02,  9.5186e-02,  1.2511e-01]],\n",
       "              \n",
       "                       [[ 7.2514e-02,  1.5427e-01,  2.4761e-02],\n",
       "                        [-2.8431e-02,  3.8902e-02,  1.2049e-02],\n",
       "                        [-5.5004e-02, -2.6223e-02,  6.5340e-03]],\n",
       "              \n",
       "                       [[-1.0293e-01, -1.2677e-01, -6.7045e-02],\n",
       "                        [-1.2148e-01, -5.9129e-02, -7.9320e-02],\n",
       "                        [-1.7049e-01, -1.0863e-01, -1.8750e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.9425e-01, -2.2385e-02,  1.9230e-01],\n",
       "                        [-2.4948e-01,  1.1875e-01,  1.5956e-01],\n",
       "                        [-1.6758e-01,  5.5870e-02,  1.0900e-01]],\n",
       "              \n",
       "                       [[-2.0010e-01,  3.7882e-02,  1.3291e-01],\n",
       "                        [-1.7094e-01,  9.9841e-02, -7.8470e-02],\n",
       "                        [ 1.7640e-02, -1.8178e-02, -6.6125e-02]],\n",
       "              \n",
       "                       [[-2.8858e-02, -4.1476e-02, -4.0651e-02],\n",
       "                        [-9.3826e-02, -8.5598e-02, -1.4781e-02],\n",
       "                        [-1.6122e-02, -7.6743e-02,  4.1185e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.3720e-01, -3.0194e-03, -3.0654e-02],\n",
       "                        [-8.0007e-02, -1.6663e-02, -2.2225e-01],\n",
       "                        [-4.8463e-03,  1.9938e-02, -1.9039e-01]],\n",
       "              \n",
       "                       [[ 8.5402e-02,  1.3420e-01,  3.3209e-02],\n",
       "                        [ 1.8063e-01,  6.6218e-02, -1.7148e-01],\n",
       "                        [ 5.5514e-02, -9.6194e-03, -1.0136e-01]],\n",
       "              \n",
       "                       [[ 5.6140e-02,  7.4467e-02,  5.1423e-02],\n",
       "                        [ 1.1885e-01, -6.6585e-02, -3.7314e-04],\n",
       "                        [-1.8740e-02, -6.1579e-02,  1.1189e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-8.7074e-02, -1.0706e-01, -1.1267e-01],\n",
       "                        [-6.6149e-02, -9.4750e-02, -1.4066e-01],\n",
       "                        [-1.5678e-03, -1.4237e-01, -2.0672e-01]],\n",
       "              \n",
       "                       [[ 4.2768e-02,  3.5428e-04,  3.0341e-02],\n",
       "                        [-6.4263e-02, -4.3710e-02, -3.2648e-02],\n",
       "                        [-9.2763e-02, -8.8467e-02, -5.2635e-02]],\n",
       "              \n",
       "                       [[-1.5771e-01, -9.5214e-02,  1.6828e-01],\n",
       "                        [-4.7780e-02, -1.4713e-01,  2.1408e-02],\n",
       "                        [ 1.6533e-02,  1.3508e-03, -1.6178e-01]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.5848e-03,  7.1103e-02,  9.1505e-02],\n",
       "                        [-5.8821e-02,  7.8604e-02,  1.3594e-01],\n",
       "                        [-1.1576e-02, -9.8423e-02,  1.4322e-01]],\n",
       "              \n",
       "                       [[ 1.9844e-01,  1.9631e-01,  1.4499e-01],\n",
       "                        [ 8.1724e-04,  1.5845e-02,  1.9721e-01],\n",
       "                        [-1.9028e-01, -4.3091e-02,  2.3232e-01]],\n",
       "              \n",
       "                       [[-9.7727e-02, -3.6270e-02, -1.0673e-01],\n",
       "                        [-2.3545e-02,  5.5673e-03, -9.2483e-02],\n",
       "                        [-1.1662e-01, -1.1180e-01, -3.3635e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-1.0521e-01, -2.2462e-02, -1.6958e-02],\n",
       "                        [ 4.0636e-02,  6.0516e-02, -4.2207e-02],\n",
       "                        [-1.0232e-01, -1.4777e-01, -6.3074e-02]],\n",
       "              \n",
       "                       [[ 1.0006e-01,  1.6165e-01,  2.0352e-01],\n",
       "                        [ 1.3098e-01,  1.6256e-01,  1.4951e-02],\n",
       "                        [-1.6656e-01,  1.8059e-02,  1.2135e-01]],\n",
       "              \n",
       "                       [[ 1.8714e-01,  1.9877e-01,  1.6197e-01],\n",
       "                        [ 8.3510e-02, -4.5603e-03,  5.5711e-03],\n",
       "                        [-5.8333e-02,  9.6655e-02, -2.0607e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 7.1041e-02,  5.1161e-02,  7.7784e-02],\n",
       "                        [-6.0839e-02,  1.1196e-01,  1.5287e-01],\n",
       "                        [-2.0091e-01, -1.8267e-01, -1.1535e-01]],\n",
       "              \n",
       "                       [[ 6.6456e-03,  7.3274e-03, -8.2627e-02],\n",
       "                        [-2.5387e-02, -2.4748e-03, -3.4398e-02],\n",
       "                        [-1.1626e-01, -1.2660e-01,  6.0339e-02]],\n",
       "              \n",
       "                       [[-2.7408e-02, -3.1325e-02, -9.4651e-02],\n",
       "                        [-1.0444e-01, -1.0892e-02, -1.2175e-01],\n",
       "                        [-1.0216e-01,  1.2464e-02, -6.6071e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.9500e-01, -2.1453e-01,  1.0682e-01],\n",
       "                        [-1.7147e-01, -3.0063e-01, -3.1151e-01],\n",
       "                        [ 9.6956e-02, -3.9043e-03, -6.1377e-02]],\n",
       "              \n",
       "                       [[-2.5671e-01, -2.4790e-01,  3.2563e-02],\n",
       "                        [ 1.4273e-02, -1.8655e-01, -3.4607e-01],\n",
       "                        [ 8.0273e-02, -6.4994e-02, -8.6351e-02]],\n",
       "              \n",
       "                       [[-5.3780e-02, -3.4204e-01, -4.1475e-01],\n",
       "                        [ 2.8656e-01,  2.3223e-01,  4.0813e-02],\n",
       "                        [ 4.2343e-02,  5.6129e-02,  2.1061e-01]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.6593e-01,  9.6423e-03,  9.3605e-02],\n",
       "                        [ 8.8772e-03, -5.9517e-02, -1.7231e-01],\n",
       "                        [ 8.8629e-02,  3.8001e-02, -1.7029e-02]],\n",
       "              \n",
       "                       [[-4.0954e-01, -2.1415e-01,  1.7180e-01],\n",
       "                        [ 1.2111e-01, -7.0311e-02, -2.0704e-01],\n",
       "                        [ 1.8310e-01,  1.5831e-01,  5.6104e-02]],\n",
       "              \n",
       "                       [[ 2.2967e-03,  9.3795e-04, -3.2731e-02],\n",
       "                        [-9.5086e-02, -4.6006e-02,  1.3733e-02],\n",
       "                        [ 5.2772e-02, -1.5006e-02,  2.2996e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.1938e-02,  6.8864e-02,  1.2303e-01],\n",
       "                        [ 6.6200e-02,  4.8246e-03, -1.0998e-01],\n",
       "                        [ 4.2803e-02, -1.1689e-01, -1.1180e-01]],\n",
       "              \n",
       "                       [[-9.6198e-02,  1.7698e-01,  9.2482e-02],\n",
       "                        [ 1.8108e-01,  1.3732e-01, -2.2669e-01],\n",
       "                        [ 6.7800e-02, -1.3558e-02,  1.7880e-01]],\n",
       "              \n",
       "                       [[-1.5570e-03, -1.1056e-01, -5.8341e-03],\n",
       "                        [ 1.5797e-02, -8.0606e-02,  1.1466e-01],\n",
       "                        [ 8.6220e-02,  1.0901e-01,  1.7445e-01]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-5.0953e-02, -2.1667e-01, -2.3014e-01],\n",
       "                        [-1.3462e-01, -1.1449e-01, -1.5971e-01],\n",
       "                        [-1.5372e-01,  6.0864e-03,  2.6628e-02]],\n",
       "              \n",
       "                       [[-1.0511e-02,  6.1615e-02, -7.7915e-02],\n",
       "                        [ 3.3797e-02,  3.0209e-03,  1.9225e-02],\n",
       "                        [-1.2454e-01, -3.7913e-03,  7.4149e-02]],\n",
       "              \n",
       "                       [[ 7.3143e-02,  8.2660e-02,  6.5769e-02],\n",
       "                        [ 4.8274e-02, -4.7877e-02, -3.8258e-04],\n",
       "                        [-8.0110e-02,  1.4451e-03, -4.0547e-02]]]])),\n",
       "             ('conv2.bias',\n",
       "              tensor([ 0.0026,  0.0974, -0.1074,  0.0279,  0.0021, -0.1227, -0.0356, -0.0457,\n",
       "                      -0.0629,  0.0546, -0.0846, -0.1150,  0.0357,  0.0289,  0.0298, -0.0497,\n",
       "                      -0.0104, -0.0010,  0.0002,  0.0126, -0.0411, -0.0854,  0.0564, -0.0780,\n",
       "                      -0.0748, -0.0178, -0.0271, -0.0538, -0.0294, -0.1107, -0.0964, -0.1163])),\n",
       "             ('fc1.weight',\n",
       "              tensor([[-0.0240, -0.0097,  0.0094,  ..., -0.0324, -0.0237, -0.0118],\n",
       "                      [-0.0152, -0.0061, -0.0042,  ..., -0.0109,  0.0307, -0.0302],\n",
       "                      [-0.0216, -0.0010,  0.0156,  ..., -0.0210,  0.0118, -0.0223],\n",
       "                      ...,\n",
       "                      [-0.0093,  0.0032,  0.0100,  ...,  0.0755,  0.0484, -0.0297],\n",
       "                      [ 0.0196, -0.0245, -0.0457,  ...,  0.0197, -0.0406,  0.0063],\n",
       "                      [ 0.0138,  0.0186,  0.0261,  ...,  0.0064, -0.0112,  0.0201]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([-0.0320,  0.0205, -0.0143,  0.0102,  0.0177, -0.0236, -0.0108,  0.0401,\n",
       "                      -0.0376, -0.0425, -0.0475, -0.0017,  0.0548,  0.0260,  0.0141, -0.0060,\n",
       "                       0.0342, -0.0174, -0.0124, -0.0101, -0.0145, -0.0152, -0.0497, -0.0422,\n",
       "                       0.0084, -0.0319, -0.0264,  0.0001, -0.0210,  0.0144, -0.0297,  0.0120,\n",
       "                       0.0161, -0.0032, -0.0141,  0.0177, -0.0267, -0.0225,  0.0137,  0.0116,\n",
       "                      -0.0220,  0.0072, -0.0161,  0.0172, -0.0308, -0.0190, -0.0205, -0.0021,\n",
       "                      -0.0249,  0.0082, -0.0166,  0.0108, -0.0435,  0.0041,  0.0391, -0.0256,\n",
       "                      -0.0296,  0.0137, -0.0211, -0.0483, -0.0162,  0.0247,  0.0188, -0.0061,\n",
       "                       0.0439, -0.0202,  0.0412, -0.0450, -0.0027,  0.0133,  0.0063, -0.0046,\n",
       "                       0.0254,  0.0190, -0.0143,  0.0008,  0.0115, -0.0083, -0.0283,  0.0231,\n",
       "                      -0.0220,  0.0226, -0.0304,  0.0506, -0.0424, -0.0234,  0.0344, -0.0200,\n",
       "                       0.0254, -0.0375,  0.0082,  0.0047,  0.0133, -0.0312, -0.0110, -0.0295,\n",
       "                       0.0241, -0.0299, -0.0072,  0.0178, -0.0561, -0.0254,  0.0295,  0.0110,\n",
       "                      -0.0154, -0.0289, -0.0259, -0.0374,  0.0018, -0.0219, -0.0313,  0.0103,\n",
       "                       0.0401, -0.0225,  0.0072,  0.0490,  0.0059, -0.0190,  0.0022,  0.0001,\n",
       "                       0.0105,  0.0707,  0.0208, -0.0159,  0.0071,  0.0443,  0.0242, -0.0697])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 0.0396, -0.0217,  0.0193,  ..., -0.3144,  0.1518, -0.0179],\n",
       "                      [ 0.1330, -0.1059,  0.0120,  ...,  0.0769,  0.1130, -0.0847],\n",
       "                      [-0.0929, -0.2256,  0.0064,  ..., -0.2107, -0.0887,  0.0502],\n",
       "                      ...,\n",
       "                      [ 0.0606,  0.0656,  0.0458,  ...,  0.1774,  0.1306,  0.0484],\n",
       "                      [ 0.0874,  0.0195,  0.0109,  ...,  0.0020, -0.0935,  0.0003],\n",
       "                      [ 0.0256, -0.1866, -0.0378,  ...,  0.1107,  0.0237,  0.0601]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([-0.0458,  0.1203, -0.0284,  0.0117,  0.0419,  0.0096,  0.0151, -0.0393,\n",
       "                      -0.0491, -0.0250]))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNNModel()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsity(mask):\n",
    "    num_zeros = torch.sum(mask == 0).item()\n",
    "    total = mask.numel()\n",
    "    return num_zeros / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cpy = CNNModel()\n",
    "population = generate_population(0.4, model_cpy, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_candidate(model, mask):\n",
    "    model_cpy = CNNModel()\n",
    "    model_cpy.load_state_dict(model.state_dict())\n",
    "    model_cpy.conv1.weight.data *= mask\n",
    "\n",
    "    # print(model_cpy.state_dict())\n",
    "\n",
    "    print(\"Sparsity: \", sparsity(mask))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model_cpy(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  0.3194444444444444\n",
      "0.9716666666666667\n",
      "Sparsity:  0.3333333333333333\n",
      "0.98075\n",
      "Sparsity:  0.3472222222222222\n",
      "0.9788333333333333\n",
      "Sparsity:  0.3333333333333333\n",
      "0.98225\n",
      "Sparsity:  0.3055555555555556\n",
      "0.9593333333333334\n"
     ]
    }
   ],
   "source": [
    "for mask in generate_population(0.4, model, 5):\n",
    "    print(evaluate_candidate(model, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutation(mask, mutation_rate):\n",
    "    # Flip random no of ones with zeroes\n",
    "    random_indices = torch.randint(0, len(mask), (int(mutation_rate * len(mask)),))\n",
    "    mask[random_indices] = 1 - mask[random_indices]\n",
    "    return mask\n",
    "\n",
    "\n",
    "def crossover(mask1, mask2):\n",
    "    # Half genes from each parent\n",
    "    new_mask = torch.zeros_like(mask1)\n",
    "    new_mask[: len(mask1) // 2] = mask1[: len(mask1) // 2]\n",
    "    new_mask[len(mask1) // 2 :] = mask2[len(mask1) // 2 :]\n",
    "\n",
    "    return new_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 1.],\n",
       "          [0., 1., 1.],\n",
       "          [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [0., 0., 1.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 0.],\n",
       "          [1., 0., 0.],\n",
       "          [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 1.],\n",
       "          [1., 0., 1.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 0.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 1.],\n",
       "          [0., 1., 1.],\n",
       "          [1., 1., 0.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 1.],\n",
       "          [1., 1., 0.],\n",
       "          [1., 0., 1.]]],\n",
       "\n",
       "\n",
       "        [[[0., 1., 0.],\n",
       "          [0., 1., 0.],\n",
       "          [0., 1., 0.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 0.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 0., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 0., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 0.],\n",
       "          [1., 0., 0.],\n",
       "          [0., 1., 0.]]],\n",
       "\n",
       "\n",
       "        [[[1., 0., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 0., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 1.],\n",
       "          [1., 1., 0.],\n",
       "          [0., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 0., 1.],\n",
       "          [0., 1., 1.],\n",
       "          [1., 0., 1.]]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population = generate_population(0.4, model, 5)\n",
    "# mutation(population[0].clone(), 0.1)\n",
    "# crossover(population[0].clone(), population[1].clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_genetic(\n",
    "    model, mutation_rate, pruning_percentage, population_size, generations\n",
    "):\n",
    "    population = generate_population(pruning_percentage, model, population_size)\n",
    "    for generation in range(generations):\n",
    "        new_population = []\n",
    "        for i in range(population_size):\n",
    "            mask1 = population[i]\n",
    "            mask2 = population[(i + 1) % population_size]\n",
    "            new_mask = crossover(mask1, mask2)\n",
    "            new_mask = mutation(new_mask, mutation_rate)\n",
    "            accuracy = evaluate_candidate(model, new_mask)\n",
    "            new_population.append((new_mask, accuracy))\n",
    "        new_population = sorted(new_population, key=lambda x: x[1], reverse=True)\n",
    "        population = [x[0] for x in new_population]\n",
    "        print(f\"Generation: {generation}, Best accuracy: {new_population[0][1]}\")\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity:  0.3888888888888889\n",
      "Sparsity:  0.3472222222222222\n",
      "Sparsity:  0.2777777777777778\n",
      "Sparsity:  0.4166666666666667\n",
      "Sparsity:  0.3333333333333333\n",
      "Generation: 0, Best accuracy: 0.9830833333333333\n",
      "Sparsity:  0.3402777777777778\n",
      "Sparsity:  0.3402777777777778\n",
      "Sparsity:  0.3055555555555556\n",
      "Sparsity:  0.3958333333333333\n",
      "Sparsity:  0.4166666666666667\n",
      "Generation: 1, Best accuracy: 0.9826666666666667\n",
      "Sparsity:  0.3472222222222222\n",
      "Sparsity:  0.3680555555555556\n",
      "Sparsity:  0.3194444444444444\n",
      "Sparsity:  0.4166666666666667\n",
      "Sparsity:  0.4375\n",
      "Generation: 2, Best accuracy: 0.982\n",
      "Sparsity:  0.3888888888888889\n",
      "Sparsity:  0.3611111111111111\n",
      "Sparsity:  0.4444444444444444\n",
      "Sparsity:  0.3888888888888889\n",
      "Sparsity:  0.4652777777777778\n",
      "Generation: 3, Best accuracy: 0.9794166666666667\n",
      "Sparsity:  0.4166666666666667\n",
      "Sparsity:  0.3958333333333333\n",
      "Sparsity:  0.3888888888888889\n",
      "Sparsity:  0.4375\n",
      "Sparsity:  0.4305555555555556\n",
      "Generation: 4, Best accuracy: 0.9685833333333334\n"
     ]
    }
   ],
   "source": [
    "population = apply_genetic(model, 0.1, 0.4, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[1., 1., 0.],\n",
       "           [1., 1., 0.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 1.],\n",
       "           [1., 1., 0.],\n",
       "           [1., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 1.],\n",
       "           [1., 1., 0.],\n",
       "           [1., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [0., 1., 1.],\n",
       "           [1., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 0.],\n",
       "           [1., 1., 0.],\n",
       "           [1., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 1., 0.],\n",
       "           [0., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 0.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 0., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 1.],\n",
       "           [0., 1., 0.],\n",
       "           [0., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 1.],\n",
       "           [0., 1., 0.],\n",
       "           [0., 0., 1.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 1.],\n",
       "           [1., 0., 0.],\n",
       "           [1., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 0.],\n",
       "           [1., 1., 0.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [1., 1., 0.],\n",
       "           [1., 0., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [0., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 1.],\n",
       "           [1., 1., 0.],\n",
       "           [1., 0., 1.]]]]),\n",
       " tensor([[[[1., 1., 0.],\n",
       "           [0., 0., 1.],\n",
       "           [1., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 1.],\n",
       "           [1., 1., 0.],\n",
       "           [0., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [0., 0., 1.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [0., 1., 0.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 1.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 1.],\n",
       "           [0., 0., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 1.],\n",
       "           [1., 0., 1.],\n",
       "           [1., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 1.],\n",
       "           [1., 0., 1.],\n",
       "           [1., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 0.],\n",
       "           [0., 1., 1.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 0.],\n",
       "           [0., 1., 1.],\n",
       "           [1., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [1., 0., 1.],\n",
       "           [1., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 1.],\n",
       "           [1., 1., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [0., 1., 1.],\n",
       "           [0., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 1.],\n",
       "           [1., 1., 0.],\n",
       "           [1., 0., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 0.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]]]]),\n",
       " tensor([[[[0., 0., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [0., 1., 1.],\n",
       "           [0., 0., 1.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 1.],\n",
       "           [0., 1., 1.],\n",
       "           [0., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 1.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 1.],\n",
       "           [1., 0., 0.],\n",
       "           [1., 0., 1.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 1., 1.],\n",
       "           [0., 0., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [1., 0., 1.],\n",
       "           [0., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 1.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 0.],\n",
       "           [1., 1., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 1.],\n",
       "           [1., 0., 1.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [0., 1., 1.]]]]),\n",
       " tensor([[[[0., 1., 1.],\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 0.],\n",
       "           [0., 0., 1.],\n",
       "           [1., 0., 1.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.],\n",
       "           [0., 1., 0.],\n",
       "           [1., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [0., 1., 1.],\n",
       "           [1., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [1., 1., 0.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 0., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [0., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 0.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 1.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 0.],\n",
       "           [0., 1., 0.],\n",
       "           [0., 0., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 0.],\n",
       "           [0., 1., 1.],\n",
       "           [0., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [0., 1., 1.],\n",
       "           [0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 0.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [1., 1., 0.],\n",
       "           [1., 1., 1.]]]]),\n",
       " tensor([[[[0., 1., 0.],\n",
       "           [1., 1., 0.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 1.],\n",
       "           [0., 0., 1.],\n",
       "           [0., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 0.],\n",
       "           [0., 1., 0.],\n",
       "           [0., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 0.],\n",
       "           [1., 0., 0.],\n",
       "           [1., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 0.],\n",
       "           [0., 0., 0.],\n",
       "           [1., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 0.],\n",
       "           [1., 1., 0.],\n",
       "           [0., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 0.],\n",
       "           [0., 1., 0.],\n",
       "           [1., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [0., 1., 1.],\n",
       "           [1., 0., 1.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 1.],\n",
       "           [1., 1., 0.],\n",
       "           [1., 0., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 0.],\n",
       "           [1., 1., 1.],\n",
       "           [1., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [1., 0., 1.],\n",
       "           [1., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 0.],\n",
       "           [1., 0., 1.],\n",
       "           [1., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 1.],\n",
       "           [1., 0., 1.],\n",
       "           [0., 1., 0.]]],\n",
       " \n",
       " \n",
       "         [[[1., 0., 1.],\n",
       "           [1., 0., 0.],\n",
       "           [0., 1., 1.]]],\n",
       " \n",
       " \n",
       "         [[[1., 1., 1.],\n",
       "           [1., 1., 0.],\n",
       "           [1., 0., 1.]]],\n",
       " \n",
       " \n",
       "         [[[0., 1., 0.],\n",
       "           [1., 1., 1.],\n",
       "           [0., 0., 1.]]]])]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model = CNNModel()\n",
    "pruned_model.load_state_dict(model.state_dict())\n",
    "\n",
    "pruned_model.conv1.weight.data *= population[0]\n",
    "\n",
    "torch.save(pruned_model.state_dict(), \"pruned_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
